{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "import pecanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Want to train a model to forecast individual household electricity usage at forecast horizons as short as 5 minutes ahead.  This forecasting model will be used to generate bids (and possibly offers!) for submission to an electricity auction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user credentials\n",
    "USER_NAME = ???\n",
    "PASSWORD = ???\n",
    "\n",
    "# define db server params\n",
    "SCHEMA = ???\n",
    "HOST = ???\n",
    "PORT = ???\n",
    "DB = ???\n",
    "\n",
    "# create the engine that connects to the database...\n",
    "engine = pecanpy.create_engine(USER_NAME, PASSWORD, HOST, PORT, DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    metadata_df = pecanpy.read_metadata_table(con, schema=SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will demonstrate how to build various RNN architectures to forecast indidividual demand for electricity. \n",
    "\n",
    "* Pure consumer\n",
    "* Prosumer\n",
    "\n",
    "Want to use households for which substantial data exists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_least_fours_years_egauge_data = (metadata_df.egauge_min_time <= \"2014-01-01\") & (metadata_df.egauge_max_time >= \"2018-01-01\")\n",
    "is_prosumer = metadata_df.pv\n",
    "consumers = metadata_df[[\"egauge_min_time\", \"egauge_max_time\"]][at_least_fours_years_egauge_data & (~is_prosumer)]\n",
    "prosumers = metadata_df[[\"egauge_min_time\", \"egauge_max_time\"]][at_least_fours_years_egauge_data & is_prosumer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data on a random prosumer from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a prosumer\n",
    "time = datetime.datetime.now()\n",
    "seed = time.hour * 10000 + time.minute * 100 + time.second\n",
    "prng = np.random.RandomState(seed)\n",
    "prosumer = prosumers.sample(n=1, random_state=prng).iloc[0, :]\n",
    "\n",
    "# alternatively, can use prosumer with dataid=8317 (very few missing obs!)\n",
    "prosumer = prosumers.loc[8317, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "\n",
    "    # download several years worth of data\n",
    "    start_time = pd.Timestamp(\"2014-01-01\", tz=\"US/Central\", freq='T')\n",
    "    end_time = pd.Timestamp(\"2018-01-01\", tz=\"US/Central\", freq='T')\n",
    "    dataid = prosumer.name\n",
    "    prosumer_egauge_df = pecanpy.read_electricity_egauge_query(con, SCHEMA, dataid, start_time, end_time, \"all\", 'T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the data to disk for safe keeping!\n",
    "prosumer_egauge_df.to_pickle(\"./prosumer_egauge_data_{}.pkl\".format(prosumer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosumer_egauge_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data on a random consumer from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime.now()\n",
    "seed = time.hour * 10000 + time.minute * 100 + time.second\n",
    "prng = np.random.RandomState(seed)\n",
    "consumer = consumers.sample(n=1, random_state=prng).iloc[0, :]\n",
    "\n",
    "# alternative use consumer with dataif=3392\n",
    "consumer = consumers.loc[3392, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "\n",
    "    # download several years worth of data\n",
    "    start_time = pd.Timestamp(\"2014-01-01\", tz=\"US/Central\", freq='T')\n",
    "    end_time = pd.Timestamp(\"2018-01-01\", tz=\"US/Central\", freq='T')\n",
    "    dataid = consumer.name\n",
    "    consumer_egauge_df = pecanpy.read_electricity_egauge_query(con, SCHEMA, dataid, start_time, end_time, \"all\", 'T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the data to disk for safe keeping!\n",
    "consumer_egauge_df.to_pickle(\"./consumer_egauge_data_{}.pkl\".format(consumer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_egauge_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Need to do a bit of preprocessing of the data before we are ready to train our RNN models.\n",
    "\n",
    "### Re-indexing the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = pd.date_range(start=start_time,\n",
    "                          end=end_time,\n",
    "                          freq='T',\n",
    "                          tz=\"US/Central\",\n",
    "                          closed=\"left\")\n",
    "reindexed_prosumer_egauge_df = prosumer_egauge_df.reindex(index=new_index, method=\"ffill\")\n",
    "reindexed_consumer_egauge_df = consumer_egauge_df.reindex(index=new_index, method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_consumer_egauge_df.use.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_prosumer_egauge_df.grid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we consider standardizing variables?\n",
    "reindexed_consumer_egauge_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_prosumer_egauge_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction target is net energy demand from the grid at some point in the future\n",
    "forecast_horizon = 5\n",
    "consumer_target = reindexed_consumer_egauge_df.grid.shift(periods=-forecast_horizon)\n",
    "prosumer_target = reindexed_prosumer_egauge_df.grid.shift(periods=-forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now shift all of the features forward to align the timestamp of the features with that of the target\n",
    "reindexed_consumer_egauge_df[\"target\"] = consumer_target\n",
    "reindexed_prosumer_egauge_df[\"target\"] = prosumer_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop or fill all remaining NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_consumer_egauge_df = (reindexed_consumer_egauge_df.fillna(axis=0, method=\"ffill\")\n",
    "                                                            .dropna(axis=1, how=\"any\")\n",
    "                                                            .drop(axis=1, labels=[\"dataid\"]))\n",
    "\n",
    "processed_prosumer_egauge_df = (reindexed_prosumer_egauge_df.fillna(axis=0, method=\"ffill\")\n",
    "                                                            .dropna(axis=1, how=\"any\")\n",
    "                                                            .drop(axis=1, labels=[\"dataid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_consumer_egauge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_prosumer_egauge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape is (n_obs, n_inputs + 1)\n",
    "processed_consumer_egauge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_prosumer_egauge_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation, and testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_test_split(df, timestamp):\n",
    "    train_idxs = df.index < timestamp\n",
    "    test_idxs = df.index >= timestamp\n",
    "    \n",
    "    # split the input dataframe into training and testing sets\n",
    "    training_df = df.loc[train_idxs]\n",
    "    testing_df = df.loc[test_idxs]\n",
    "    \n",
    "    return training_df, testing_df\n",
    "\n",
    "\n",
    "def _train_validation_test_split(df, timestamp1, timestamp2):\n",
    "    assert timestamp1 < timestamp2\n",
    "    train_idxs = df.index < timestamp1\n",
    "    validation_idxs = (df.index >= timestamp1) & (df.index < timestamp2)\n",
    "    test_idxs = df.index >= timestamp2\n",
    "    \n",
    "    # split the input dataframe into training, validation, and testing sets\n",
    "    training_df = df.loc[train_idxs]\n",
    "    validation_df = df.loc[validation_idxs]\n",
    "    testing_df = df.loc[test_idxs]\n",
    "    \n",
    "    return training_df, validation_df, testing_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on first 3 year of data and use the final year as the testing data\n",
    "n_time_steps = 24 * 60\n",
    "training_end_time = start_time + (365 + 365 + 366) * n_time_steps\n",
    "consumer_training_df, consumer_testing_df = _train_test_split(processed_consumer_egauge_df, training_end_time)\n",
    "prosumer_training_df, prosumer_testing_df = _train_test_split(processed_prosumer_egauge_df, training_end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len of three splits must be multiples of n_timesteps!\n",
    "assert len(consumer_training_df) % n_time_steps == 0\n",
    "assert len(consumer_testing_df) % n_time_steps == 0\n",
    "\n",
    "assert len(prosumer_training_df) % n_time_steps == 0\n",
    "assert len(prosumer_testing_df) % n_time_steps == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosumer_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_training_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_training_features_df = consumer_training_df.drop(axis=1, labels=\"target\", inplace=False)                                \n",
    "consumer_training_target = consumer_training_df.target\n",
    "\n",
    "consumer_testing_features_df = consumer_testing_df.drop(axis=1, labels=\"target\", inplace=False)\n",
    "consumer_testing_target = consumer_testing_df.target\n",
    "\n",
    "prosumer_training_features_df = prosumer_training_df.drop(axis=1, labels=\"target\", inplace=False)                                \n",
    "prosumer_training_target = prosumer_training_df.target\n",
    "\n",
    "prosumer_testing_features_df = prosumer_testing_df.drop(axis=1, labels=\"target\", inplace=False)\n",
    "prosumer_testing_target = prosumer_testing_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_next_training_batch(training_features_df, training_target, batch_idx, batch_size, n_steps, n_inputs, n_outputs):\n",
    "    start = batch_idx * batch_size * n_steps\n",
    "    stop = start + batch_size * n_steps\n",
    "    X = training_features_df.values[start:stop]\n",
    "    y = training_target.values[start:stop]\n",
    "    return X.reshape(-1, n_steps, n_inputs), y.reshape(-1, n_steps, n_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.placeholder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rnn(n_inputs, n_outputs=1):\n",
    "    graph = tf.Graph()\n",
    "    X = tf.placeholder(tf.float64, [None, n_time_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.float64, [None, n_time_steps, n_outputs])\n",
    "\n",
    "    # multiple RNN layers\n",
    "    n_layers = 2\n",
    "    n_neurons = 20\n",
    "    rnn_layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.tanh) for layer in range(n_layers)]\n",
    "    multi_layer_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers) \n",
    "    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float64)\n",
    "\n",
    "    # use a single dense layer to reduce the dimensionality\n",
    "    stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "    stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "    y_hat = tf.reshape(stacked_outputs, [-1, n_time_steps, n_outputs])\n",
    "\n",
    "    # define the loss function and an optimizer\n",
    "    error = y_hat - y\n",
    "    mean_square_error = tf.reduce_mean(tf.square(error))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(mean_square_error, name=\"training_op\")\n",
    "\n",
    "    # create a saver to store model output\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rnn_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_epochs = 150\n",
    "\n",
    "batch_size = 100\n",
    "n_training_obs, _ = consumer_training_df.shape \n",
    "n_training_batches = ((n_training_obs // n_time_steps) // batch_size) + 1\n",
    "\n",
    "basic_rnn_graph = basic_rnn(consumer_training_features_df.shape[1])\n",
    "\n",
    "with tf.Session(basic_rnn_graph) as sess:\n",
    "    init.run()\n",
    "    for i in range(n_training_epochs):\n",
    "        for j in range(n_training_batches):\n",
    "            X_batch, y_batch = _fetch_next_training_batch(consumer_training_features_df, consumer_training_target, j, batch_size, n_time_steps, n_inputs, n_outputs)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss = mean_square_error.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        if i % 10 == 0:\n",
    "            print(\"After {} training epochs, the MSE on most recent batch is {}.\".format(i, loss))\n",
    "\n",
    "    saver.save(sess, \"./trained-models/basic_rnn_load_forecasting_model_for_{}\".format(consumer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# placeholders to fill with data eventually\n",
    "_, n_cols = consumer_training_df.shape\n",
    "n_outputs = 1\n",
    "n_inputs = n_cols - n_outputs\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, n_time_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float64, [None, n_time_steps, n_outputs])\n",
    "\n",
    "# multiple RNN layers\n",
    "n_layers = 2\n",
    "n_neurons = 20\n",
    "rnn_layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.tanh) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers) \n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float64)\n",
    "\n",
    "# use a single dense layer to reduce the dimensionality\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "y_hat = tf.reshape(stacked_outputs, [-1, n_time_steps, n_outputs])\n",
    "\n",
    "# define the loss function and an optimizer\n",
    "error = y_hat - y\n",
    "mean_square_error = tf.reduce_mean(tf.square(error))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(mean_square_error, name=\"training_op\")\n",
    "\n",
    "# create a saver to store model output\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_epochs = 150\n",
    "\n",
    "batch_size = 100\n",
    "n_training_obs, _ = consumer_training_df.shape \n",
    "n_training_batches = ((n_training_obs // n_time_steps) // batch_size) + 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(n_training_epochs):\n",
    "        for j in range(n_training_batches):\n",
    "            X_batch, y_batch = _fetch_next_training_batch(consumer_training_features_df, consumer_training_target, j, batch_size, n_time_steps, n_inputs, n_outputs)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss = mean_square_error.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        if i % 10 == 0:\n",
    "            print(\"After {} training epochs, the MSE on most recent batch is {}.\".format(i, loss))\n",
    "\n",
    "    saver.save(sess, \"./trained-models/basic_rnn_load_forecasting_model_for_{}\".format(consumer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                          \n",
    "    saver.restore(sess, \"./trained-models/basic_rnn_load_forecasting_model_for_{}\".format(consumer.name))\n",
    "    X_test = consumer_testing_features_df.values\n",
    "    y_test = consumer_testing_target.values\n",
    "    rnn_predictions, rnn_mse = sess.run([y_hat, mean_square_error], feed_dict={X: X_test.reshape(-1, n_time_steps, n_inputs), y: y_test.reshape(-1, n_time_steps, n_outputs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = pd.Series(rnn_predictions.ravel(),\n",
    "                                index=consumer_testing_target.index,\n",
    "                                name=\"predictions\")\n",
    "rnn_results_df = pd.concat([consumer_testing_target, testing_predictions], join=\"inner\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "rnn_results_df.head(60 * 24).plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.ma.masked_invalid(np.abs((y_true - y_pred) / y_true)).mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(rnn_results_df.target, rnn_results_df.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Units\n",
    "\n",
    "![A Long Short-Term Memory (LSTM) Unit](./assets/greff_lstm_diagram.png)\n",
    "\n",
    "Image by Klaus Greff and colleagues as published in [*LSTM: A Search Space Odyssey*](https://arxiv.org/abs/1503.04069)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# placeholders to fill with data eventually\n",
    "X = tf.placeholder(tf.float64, [None, n_time_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float64, [None, n_time_steps, n_outputs])\n",
    "\n",
    "# multiple LSTM layers with peep-hole connections\n",
    "lstm_layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(lstm_layers) \n",
    "lstm_outputs, lstm_states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float64)\n",
    "\n",
    "# use a single dense layer to reduce the dimensionality\n",
    "stacked_lstm_outputs = tf.reshape(lstm_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_lstm_outputs, n_outputs)\n",
    "y_hat = tf.reshape(stacked_outputs, [-1, n_time_steps, n_outputs])\n",
    "\n",
    "# define the loss function and an optimizer\n",
    "error = y_hat - y\n",
    "mean_square_error = tf.reduce_mean(tf.square(error))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(mean_square_error)\n",
    "\n",
    "# create a saver to store model output\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(n_training_epochs):\n",
    "        for j in range(n_training_batches):\n",
    "            X_batch, y_batch = _fetch_next_training_batch(j, batch_size, n_time_steps, n_inputs, n_outputs)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss = mean_square_error.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        if i % 10 == 0:\n",
    "            print(\"After {} training epochs, the MSE on the most recent batch is {}.\".format(i, loss))\n",
    "\n",
    "    saver.save(sess, \"./lstm_rnn_load_forecasting_model_for_{}\".format(dataid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                          \n",
    "    saver.restore(sess, \"./lstm_rnn_load_forecasting_model_for_{}\".format(dataid))\n",
    "    X_test = testing_features_df.values\n",
    "    y_test = testing_target.values\n",
    "    lstm_predictions, lstm_mse = sess.run([y_hat, mean_square_error], \n",
    "                                          feed_dict={X: X_test.reshape(-1, n_time_steps, n_inputs), y: y_test.reshape(-1, n_time_steps, n_outputs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = pd.Series(lstm_predictions.ravel(),\n",
    "                                index=testing_target.index,\n",
    "                                name=\"predictions\")\n",
    "lstm_results_df = pd.concat([testing_target, testing_predictions], join=\"inner\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "lstm_results_df.head(60 * 24).plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(lstm_results_df.target, lstm_results_df.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRU)\n",
    "\n",
    "![A Long Short-Term Memory (LSTM) Unit](./assets/lstm_gru.png)\n",
    "\n",
    "Image by Klaus Greff and colleagues as published in [*Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*](https://arxiv.org/abs/1412.3555). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# placeholders to fill with data eventually\n",
    "X = tf.placeholder(tf.float64, [None, n_time_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float64, [None, n_time_steps, n_outputs])\n",
    "\n",
    "# multiple GRU layers\n",
    "gru_layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(gru_layers) \n",
    "gru_outputs, gru_states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float64)\n",
    "\n",
    "# use a single dense layer to reduce the dimensionality\n",
    "stacked_gru_outputs = tf.reshape(gru_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_gru_outputs, n_outputs)\n",
    "y_hat = tf.reshape(stacked_outputs, [-1, n_time_steps, n_outputs])\n",
    "\n",
    "# define the loss function and an optimizer\n",
    "error = y_hat - y\n",
    "mean_square_error = tf.reduce_mean(tf.square(error))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(mean_square_error)\n",
    "\n",
    "# create a saver to store model output\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(n_training_epochs):\n",
    "        for j in range(n_training_batches):\n",
    "            X_batch, y_batch = _fetch_next_training_batch(j, batch_size, n_time_steps, n_inputs, n_outputs)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss = mean_square_error.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        if i % 10 == 0:\n",
    "            print(\"After {} training epochs, the MSE on the most recent batch is {}.\".format(i, loss))\n",
    "\n",
    "    saver.save(sess, \"./gru_rnn_load_forecasting_model_for_{}\".format(dataid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                          \n",
    "    saver.restore(sess, \"./gru_rnn_load_forecasting_model_for_{}\".format(dataid))\n",
    "    X_test = testing_features_df.values\n",
    "    y_test = testing_target.values\n",
    "    gru_predictions, gru_mse = sess.run([y_hat, mean_square_error],\n",
    "                                        feed_dict={X: X_test.reshape(-1, n_time_steps, n_inputs), y: y_test.reshape(-1, n_time_steps, n_outputs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = pd.Series(gru_predictions.ravel(),\n",
    "                                index=testing_target.index,\n",
    "                                name=\"predictions\")\n",
    "gru_results_df = pd.concat([testing_target, testing_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "gru_results_df.head(60 * 24).plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(gru_results_df.target, gru_results_df.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
